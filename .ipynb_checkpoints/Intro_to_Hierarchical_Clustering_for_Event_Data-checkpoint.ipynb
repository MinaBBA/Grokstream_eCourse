{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "transsexual-level",
   "metadata": {},
   "source": [
    "## Introduction to Hierarchical Clustering for Event Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-teaching",
   "metadata": {},
   "source": [
    "#### Section I: \n",
    "How to implement three common clustering algorithms (Kmeans,DBSCAN, and Agglomertive Hirerarchical clustering) in python\n",
    "\n",
    "In this section we demonstrate characteristics of three common clustering algorithms on toy datasets. Human eyes are really good at clusetring 2D data, so this exercise would allow one to get an intuition on \n",
    "\n",
    "    1- how each clustering algorithm works\n",
    "    2- how to tune clustering parameters to improve clustering accuracy\n",
    "    3- compare performance of different clustering algorithms for given datasets  \n",
    "\n",
    "More clustering examples on the toy datasets can be found in the link below. As much as this examples are interesting, they are still in 2D and so the intuition might not apply to very high dimensional data!\n",
    "\n",
    "To run each cell of the notebook you can click on the cell to select it, then press (SHIFT+ENTER) , or (CTRL+ENTER), or press the play button in the toolbar above. \n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-affair",
   "metadata": {},
   "source": [
    "Load required libraries for generating toy dataset and plotting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster, datasets \n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-stone",
   "metadata": {},
   "source": [
    "Generate datasets. We choose the size big enough to see the scalability of the algorithms, but not too big to avoid too long running times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,noise=.05)\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "no_structure = np.random.rand(n_samples, 2), None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-boating",
   "metadata": {},
   "source": [
    "Plot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=['blobs','aniso','blobs with varied variances','noisy_circles','noisy_moons','no_structure']\n",
    "dataset_list = [\n",
    "                blobs,\n",
    "                aniso,\n",
    "                varied,\n",
    "                noisy_circles,\n",
    "                noisy_moons,\n",
    "                no_structure\n",
    "                ]\n",
    "plt.figure(figsize=(30,5))\n",
    "for i,dataset in enumerate(dataset_list):\n",
    "    i+=1\n",
    "    X=dataset[0]\n",
    "    plt.subplot(1,len(dataset_list),i) \n",
    "    plt.scatter(X[:, 0], X[:, 1],c='black')\n",
    "    plt.title(str(dataset_names[i-1]),fontsize=20)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-bishop",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-fireplace",
   "metadata": {},
   "source": [
    "#### steps of K-means clustering algorithm\n",
    "    1- specify the number of clusters (K) to assign.\n",
    "    2- randomly initiate k centroids\n",
    "    3- repeat:\n",
    "        4- assing each point to its closest cetroid\n",
    "        5- calculate the new centroid of each cluster\n",
    "    6- until the centroid positions do not change\n",
    "    \n",
    "main hyperparameter: number k of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules for Kmeans clustering \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize clustering parameters\n",
    "datasets_list = [\n",
    "    (blobs, {'n_clusters': 3}),\n",
    "    (aniso, {'n_clusters': 3}),\n",
    "    (varied, {'n_clusters': 3}),\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (no_structure, {'n_clusters': 3})]\n",
    "\n",
    "default_base = {'n_clusters': 3}\n",
    "\n",
    "plt.figure(figsize=(30,5))\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets_list):\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "    \n",
    "    X,y=dataset\n",
    "    \n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Create cluster objects\n",
    "    kmeans = KMeans(n_clusters=algo_params['n_clusters'],\n",
    "                    init=\"random\",\n",
    "                    n_init=10,\n",
    "                    max_iter=300,\n",
    "                    random_state=42\n",
    "                    )\n",
    "    # Train clustering model\n",
    "    kmeans.fit(X)\n",
    "    centroids=kmeans.cluster_centers_[:, 1]\n",
    "    # Plot dataset and its cluater centroids \n",
    "    plt.subplot(1,len(dataset_list),i_dataset+1) \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, s=50, cmap='viridis')\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], centroids, c='red', s=200, alpha=0.5)\n",
    "    plt.title(str(dataset_names[i_dataset]),fontsize=20)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-tragedy",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "    1- k-means is fast and simple\n",
    "    2- the result depends on starting guesses (k, initial states of centroids)\n",
    "    2- is limited to linear clustering boundaries\n",
    "    3- performs well on clusters with Gaussian distribution, but faild if clusters have complex geometries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-poverty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "leading-communication",
   "metadata": {},
   "source": [
    "## Aglomerative Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-frame",
   "metadata": {},
   "source": [
    "#### - Agglomerative clustering is the bottom-up approach. \n",
    "    It starts by considering each datapoint as a single cluster. Then merges the two points(single clusters) that are the most similar until all points have been merged into a single cluster.\n",
    "\n",
    "#### - Divisive clustering is the top-down approach.\n",
    "    It starts with all points as one giant cluster and splits the least similar clusters at each step until only single data points remain.\n",
    "\n",
    "Both approaches generate a cluster tree (dendogram) that shows the hierachical relationship between similar sets of data.\n",
    "\n",
    "The main hyperparameters here are linkage_method, and distance/similarity threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-amber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules for AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Comparing characteristics of different linkage methods for Agglomerative hierarchical clustering on 2D datasets :')\n",
    "# initialize clustering parameters\n",
    "datasets_list = [\n",
    "    (blobs, {'n_clusters': 3}),\n",
    "    (aniso, {'n_neighbors': 3}),\n",
    "    (varied, {'n_neighbors': 3}),\n",
    "    (noisy_circles, {'n_clusters': 2}),\n",
    "    (noisy_moons, {'n_clusters': 2}),\n",
    "    (no_structure, {'n_clusters': 3})]\n",
    "\n",
    "default_base = {'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "for i_linkage,linkage in enumerate(['single','complete','average','ward']):\n",
    "    \n",
    "    print('\\n linkage method : ', linkage)\n",
    "    plt.figure(figsize=(30,5))\n",
    "    \n",
    "    for i_dataset, (dataset, algo_params) in enumerate(datasets_list):\n",
    "        # update parameters with dataset-specific values\n",
    "        params = default_base.copy()\n",
    "        params.update(algo_params)\n",
    "\n",
    "        X,y=dataset\n",
    "\n",
    "        # normalize dataset for easier parameter selection\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Create cluster objects\n",
    "        algorithm = AgglomerativeClustering(\n",
    "                        n_clusters=params['n_clusters'], linkage=linkage)\n",
    "\n",
    "        # Train clustering model\n",
    "        algorithm.fit(X)\n",
    "        y_pred = algorithm.labels_.astype(int)\n",
    "        # Plot dataset and its cluater centroids \n",
    "        plt.subplot(1,len(dataset_list),i_dataset+1) \n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "        plt.title(str(dataset_names[i_dataset]),fontsize=20)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Visualize dendogram of hierarchical clustering')\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "X = StandardScaler().fit_transform(datasets.make_moons(n_samples=150, noise=.05)[0])\n",
    "for linkage_method in ['ward','single','complete']:\n",
    "    metric='euclidean'\n",
    "    Z = linkage(X, method=linkage_method,metric=metric)\n",
    "    plt.figure(figsize=(30,5))\n",
    "    dendrogram(Z) \n",
    "    plt.title('Dendogram representation of the dataset with 150 data points. linkage :'+linkage_method,fontsize=20)\n",
    "    plt.ylabel('distance')\n",
    "    plt.xlabel('data points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-compensation",
   "metadata": {},
   "source": [
    "#### The main observations to make are:\n",
    "\n",
    "single linkage is fast, and can perform well on non-globular data, but it performs poorly in the presence of noise.\n",
    "\n",
    "average and complete linkage perform well on cleanly separated globular clusters, but have mixed results otherwise.\n",
    "\n",
    "Ward is the most effective method for noisy data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "monetary-administrator",
   "metadata": {},
   "source": [
    "## Density based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-smoke",
   "metadata": {},
   "source": [
    "### DBSCAN groups together closely-packed points. \n",
    "\n",
    "- We donâ€™t need to specify the number of clusters\n",
    "- Flexibility in the shapes & sizes of clusters\n",
    "- Able to deal with noise and outliers\n",
    "- Border may be arbitrarily classified - not an issue for most datasets\n",
    "- Faces difficulty finding clusters of varying densities \n",
    "\n",
    "There are two inputs to DBSCAN:\n",
    "- esp: the search distance around point, and\n",
    "- minpts: the minimum number of points required to form a density cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = [\n",
    "    (blobs, {}),\n",
    "    (aniso, {'eps': .15}),\n",
    "    (varied, {'eps': .15}),\n",
    "    (noisy_circles, {}),\n",
    "    (noisy_moons, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "default_base = {'eps': .3}\n",
    "\n",
    "plt.figure(figsize=(30,5))\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets_list):\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "    \n",
    "    X,y=dataset\n",
    "    \n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Create cluster objects\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    \n",
    "    # Train clustering model\n",
    "    dbscan.fit(X)\n",
    "    y_pred = dbscan.labels_.astype(int)\n",
    "    \n",
    "    plt.subplot(1,len(dataset_list),i_dataset+1) \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=50, cmap='viridis')\n",
    "    plt.title(str(dataset_names[i_dataset]),fontsize=20)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-batman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "expected-knight",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering of event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from fastcluster import linkage as linkage2\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.spatial.distance import squareform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sample event data to be clusterd\n",
    "events=pd.read_csv('example/event_data.csv')\n",
    "print(events[['Node','Event_Type','Pattern','inc_id']].nunique())\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load time series database of the historical data\n",
    "pattern_ts=pd.read_csv('example/pattern_timeseries.csv')\n",
    "# load representational memory (pattern distance matrix) \n",
    "dm=pd.read_csv('example/pattern_dm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a clustring model using representational memory of pattren dimension\n",
    "\n",
    "# select span window to group events of the sample dataset\n",
    "t_range=['2021-01-26 06:00:01','2021-01-26 18:00:01']\n",
    "test=events[(events['t'] >= t_range[0]) & (events['t'] < t_range[1])]\n",
    "\n",
    "# define clustering parameters\n",
    "level='Pattern'\n",
    "linkage_method='complete'\n",
    "criterion='distance'\n",
    "d=0.7\n",
    "dm=[dm.values,dm.columns]\n",
    "# retrieve pairwise distance values bewteen the pattern time series of the test dataset\n",
    "u_values = test[level].unique()\n",
    "overlap_values = list(set(u_values) & set(dm[1]))\n",
    "if len(overlap_values) >= 2:\n",
    "    overlap_values_index = list(np.in1d(list(dm[1]), overlap_values).nonzero()[0])\n",
    "    reordered_overlap_values = np.array(dm[1])[overlap_values_index] \n",
    "else:\n",
    "    print(\"representational memory canot provide enough information to train the model for the given span window\")\n",
    "sub_dm = dm[0][np.ix_(overlap_values_index, overlap_values_index)]\n",
    "\n",
    "# train clustering model\n",
    "z = linkage2(squareform(sub_dm), method=linkage_method)\n",
    "results = fcluster(z, d, criterion=criterion)\n",
    "tscol_clusterID = pd.DataFrame({level: reordered_overlap_values, 'clusterID': results})\n",
    "results_df = pd.merge(tscol_clusterID, tscol_clusterID.groupby('clusterID').count().reset_index().rename(columns={level:'members'}),\n",
    "                          on='clusterID')\n",
    "print(results_df.shape)\n",
    "results_df[results_df['members']>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag clustered events of the test with their clusterID\n",
    "test=test.merge(results_df[results_df['members']>1][[level,'clusterID']], on=level, how='left')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_analysis(inc):\n",
    "    print('list of clusterID(s)',test[test['inc_id']==inc]['clusterID'].unique())\n",
    "    print('events tagged by inc_id: ',inc)\n",
    "    display(test[test['inc_id']==inc])\n",
    "    cids=test[(test['inc_id']==inc) & (~test['clusterID'].isna())]['clusterID'].unique()\n",
    "    print('============')\n",
    "    for cid in cids:\n",
    "        subdf=test[test['clusterID'].isin(cids)]\n",
    "        print('clusterID %s has \\n'%(cid),subdf[['Pattern','Event_Type','Node','inc_id']].nunique())\n",
    "        print('events of clusterID ',cid)\n",
    "        display(subdf)\n",
    "    print('plot pattern time series of the events tagged with inc_id ',inc)\n",
    "    pattern_ts[test[test['clusterID'].isin(cids)][level].unique()].plot(figsize=(25,5))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_analysis('INC000000888346')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_analysis('INC000000895068')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_analysis('INC000000895349')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-retrieval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-savage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
